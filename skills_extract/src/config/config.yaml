# =============================================================================
# Skills Extraction Pipeline Configuration
# =============================================================================
# Cookiecutter Data Science Structure
# This file controls all pipeline behavior. Modify values here instead of code.
# =============================================================================

# -----------------------------------------------------------------------------
# Pipeline Metadata
# -----------------------------------------------------------------------------
pipeline:
  name: "skills_extraction_pipeline"
  version: "1.0.0"

# -----------------------------------------------------------------------------
# Input Configuration
# -----------------------------------------------------------------------------
input:
  # Path to your input file (relative to project root)
  file_path: C:\Users\thann\Downloads\skills_extract\skills_extract\data\raw\job_descriptions.csv
  
  # File type: "auto" (detect from extension), "csv", or "xlsx"
  file_type: "auto"
  
  # Encoding: "auto" (auto-detect), "utf-8", "latin-1", etc.
  encoding: "auto"
  
  # Column mappings - CHANGE THESE FOR YOUR DATASET
  columns:
    # REQUIRED: Column containing job description text
    text_column: "jd"
    
    # OPTIONAL: Column for unique ID (null = auto-generate as jd_001, jd_002, etc.)
    id_column: null
    
    # OPTIONAL: Column for job title
    title_column: null
    
    # OPTIONAL: Column for role/category (used in Phase 2 analytics)
    role_column: null
  
  # Auto-generated ID prefix (when id_column is null)
  id_prefix: "jd"
  
  # Chunk size for processing large files (rows per chunk)
  chunk_size: 5000
  
  # File size threshold (MB) for automatic chunked loading
  chunk_threshold_mb: 100

# -----------------------------------------------------------------------------
# Preprocessing Configuration
# -----------------------------------------------------------------------------
preprocessing:
  # Convert text to lowercase
  lowercase: true
  
  # Normalize whitespace (multiple spaces → single space)
  normalize_whitespace: true
  
  # Remove special characters
  remove_special_chars: true
  
  # Preserve hyphens in skills like "CI-CD", "full-stack"
  preserve_hyphens: true
  
  # Expand abbreviations (mgmt → management)
  expand_abbreviations: true
  abbreviations_file: "src/config/abbreviations.yaml"

# -----------------------------------------------------------------------------
# Extraction Configuration
# -----------------------------------------------------------------------------
extraction:
  # Skill dictionary files (relative to project root)
  dictionaries:
    technical: "references/dictionaries/technical_skills.txt"
    soft: "references/dictionaries/soft_skills.txt"
    # Add your custom skills file here (optional)
    custom: null
  
  # Skill normalization (JS → JavaScript)
  normalize_skills: true
  variations_file: "references/dictionaries/skill_variations.yaml"
  
  # Matching settings
  case_sensitive: false
  remove_duplicates: true

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
output:
  # Output directory (relative to project root)
  directory: "data/processed"
  
  # Output formats to generate
  formats:
    - csv
    - json
  
  # CSV output settings
  csv:
    filename: "extracted_skills.csv"
    # Format for skills column: "comma_separated" or "json_array"
    skills_format: "comma_separated"
    # Columns to include in output
    include_columns:
      - job_id
      - job_title
      - extracted_skills
      - skill_count
  
  # JSON output settings
  json:
    filename: "extracted_skills.json"
    indent: 2

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Log file path (relative to project root)
  file: "reports/pipeline.log"
  
  # Also log to console
  console: true
  
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# -----------------------------------------------------------------------------
# Analytics Configuration (Phase 2)
# -----------------------------------------------------------------------------
analytics:
  # Enable analytics generation
  enabled: false
  
  # Number of top skills to rank
  top_n_skills: 20
  
  # Minimum frequency to include in analytics
  min_frequency: 1
  
  # Output directory for analytics files
  output_dir: "reports/analytics"
  
  # Role normalization mappings (group similar titles)
  role_mappings:
    "Data Engineer":
      - "data engineer"
      - "sr data engineer"
      - "senior data engineer"
      - "data engineering"
    "Data Scientist":
      - "data scientist"
      - "sr data scientist"
      - "senior data scientist"

# -----------------------------------------------------------------------------
# Rules Configuration (Phase 2)
# -----------------------------------------------------------------------------
rules:
  # Enable rules generation
  enabled: false
  
  # Minimum frequency to include in rules
  min_frequency_threshold: 2
  
  # Core skill threshold (percentage of JDs)
  # Skills appearing in > 50% of JDs are marked as "core"
  core_skill_threshold: 0.5
  
  # Output directory for rule files
  output_dir: "reports/rules"
